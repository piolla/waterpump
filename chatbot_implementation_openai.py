import streamlit as st
import json
import pandas as pd
import numpy as np
from datetime import datetime
import plotly.express as px
import plotly.graph_objects as go
import requests
import os
from water_pump_analyzer import WaterPumpAnalyzer

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì›Œí„°íŒí”„ AI ë¶„ì„ ì±—ë´‡ (LLM)",
    page_icon="ğŸ¤–",
    layout="wide"
)

class LLMWaterPumpChatbot:
    def __init__(self):
        self.data = None
        self.analysis_cache = {}
        self.llm_provider = None
        self.openai_api_key = None
        self.ollama_model = None
        
    def set_llm_provider(self, provider, **kwargs):
        """LLM ì œê³µì ì„¤ì •"""
        self.llm_provider = provider
        if provider == "openai":
            self.openai_api_key = kwargs.get("api_key")
        elif provider == "ollama":
            self.ollama_model = kwargs.get("model", "llama2")
    
    def load_json_data(self, uploaded_file):
        """JSON ë°ì´í„° ë¡œë“œ"""
        try:
            self.data = json.load(uploaded_file)
            self.analyze_data()
            return True
        except Exception as e:
            st.error(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False
    
    def analyze_data(self):
        """ë°ì´í„° ë¶„ì„ ë° ìºì‹œ ìƒì„±"""
        if not self.data:
            return
            
        results = self.data['analysis_results']
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        all_temps = []
        alert_counts = {'ì •ìƒ': 0, 'ê´€ì°°': 0, 'ì£¼ì˜': 0, 'ìœ„í—˜': 0}
        trend_counts = {'ìƒìŠ¹': 0, 'í•˜ê°•': 0, 'í‰í˜•': 0}
        stability_counts = {'ë§¤ìš°ì•ˆì •': 0, 'ì•ˆì •': 0, 'ë³´í†µ': 0, 'ë¶ˆì•ˆì •': 0}
        
        critical_batches = []
        
        for result in results:
            all_temps.append(result['statistics']['mean'])
            alert_counts[result['alert_level']] += 1
            trend_counts[result['trend']] += 1
            stability_counts[result['stability']] += 1
            
            # ìœ„í—˜/ì£¼ì˜ ë°°ì¹˜ ì‹ë³„
            if result['alert_level'] in ['ìœ„í—˜', 'ì£¼ì˜']:
                critical_batches.append(result)
        
        self.analysis_cache = {
            'total_batches': len(results),
            'avg_temperature': np.mean(all_temps),
            'max_temperature': max([r['statistics']['max'] for r in results]),
            'min_temperature': min([r['statistics']['min'] for r in results]),
            'alert_counts': alert_counts,
            'trend_counts': trend_counts,
            'stability_counts': stability_counts,
            'critical_batches': critical_batches,
            'analysis_period': {
                'start': results[0]['start_timestamp'],
                'end': results[-1]['end_timestamp']
            }
        }
    
    def create_context_prompt(self, user_query):
        """JSON ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        if not self.data or not self.analysis_cache:
            return "ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        # ë°ì´í„° ìš”ì•½ ìƒì„±
        cache = self.analysis_cache
        metadata = self.data['metadata']
        
        context = f"""
ë‹¹ì‹ ì€ ì›Œí„°íŒí”„ ì˜¨ë„ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

## ë°ì´í„° ê°œìš”
- ë¶„ì„ ê¸°ê°„: {cache['analysis_period']['start'][:10]} ~ {cache['analysis_period']['end'][:10]}
- ì´ ë°°ì¹˜ ìˆ˜: {cache['total_batches']}ê°œ
- ìœˆë„ìš° í¬ê¸°: {metadata.get('window_size', 100)} ë ˆì½”ë“œ
- ë°ì´í„° ì†ŒìŠ¤: {metadata.get('data_source', 'N/A')}

## ì˜¨ë„ í†µê³„
- í‰ê·  ì˜¨ë„: {cache['avg_temperature']:.1f}Â°C
- ìµœê³  ì˜¨ë„: {cache['max_temperature']:.1f}Â°C
- ìµœì € ì˜¨ë„: {cache['min_temperature']:.1f}Â°C

## ê²½ê³  ìˆ˜ì¤€ ë¶„í¬
- ì •ìƒ: {cache['alert_counts']['ì •ìƒ']}ê°œ ({(cache['alert_counts']['ì •ìƒ']/cache['total_batches']*100):.1f}%)
- ê´€ì°°: {cache['alert_counts']['ê´€ì°°']}ê°œ ({(cache['alert_counts']['ê´€ì°°']/cache['total_batches']*100):.1f}%)
- ì£¼ì˜: {cache['alert_counts']['ì£¼ì˜']}ê°œ ({(cache['alert_counts']['ì£¼ì˜']/cache['total_batches']*100):.1f}%)
- ìœ„í—˜: {cache['alert_counts']['ìœ„í—˜']}ê°œ ({(cache['alert_counts']['ìœ„í—˜']/cache['total_batches']*100):.1f}%)

## íŠ¸ë Œë“œ ë¶„í¬
- ìƒìŠ¹: {cache['trend_counts']['ìƒìŠ¹']}ê°œ
- í•˜ê°•: {cache['trend_counts']['í•˜ê°•']}ê°œ
- í‰í˜•: {cache['trend_counts']['í‰í˜•']}ê°œ

## ì•ˆì •ì„± ë¶„í¬
- ë§¤ìš°ì•ˆì •: {cache['stability_counts']['ë§¤ìš°ì•ˆì •']}ê°œ
- ì•ˆì •: {cache['stability_counts']['ì•ˆì •']}ê°œ
- ë³´í†µ: {cache['stability_counts']['ë³´í†µ']}ê°œ
- ë¶ˆì•ˆì •: {cache['stability_counts']['ë¶ˆì•ˆì •']}ê°œ

## ìœ„í—˜ ë°°ì¹˜ ìƒì„¸
"""
        
        # ìœ„í—˜ ë°°ì¹˜ ì •ë³´ ì¶”ê°€
        if cache['critical_batches']:
            context += f"ìœ„í—˜/ì£¼ì˜ ë°°ì¹˜: {len(cache['critical_batches'])}ê°œ\n"
            for batch in cache['critical_batches'][:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                context += f"- ë°°ì¹˜ {batch['batch_id']}: {batch['statistics']['mean']:.1f}Â°C ({batch['alert_level']}, {batch['value_label']})\n"
        else:
            context += "í˜„ì¬ ìœ„í—˜ ë˜ëŠ” ì£¼ì˜ ë°°ì¹˜ ì—†ìŒ\n"
        
        # ìƒì„¸ ë¶„ì„ ë°ì´í„° (ìµœê·¼ 5ê°œ ë°°ì¹˜)
        context += "\n## ìµœê·¼ ë°°ì¹˜ ìƒì„¸ ë¶„ì„\n"
        recent_batches = self.data['analysis_results'][-5:]
        for batch in recent_batches:
            context += f"""
ë°°ì¹˜ {batch['batch_id']}:
- ì‹œê°„: {batch['start_timestamp'][:16]}
- í‰ê·  ì˜¨ë„: {batch['statistics']['mean']:.1f}Â°C
- ì˜¨ë„ ë²”ìœ„: {batch['statistics']['min']:.1f}Â°C ~ {batch['statistics']['max']:.1f}Â°C
- í‘œì¤€í¸ì°¨: {batch['statistics']['std']:.1f}Â°C
- íŠ¹ì„± ë¼ë²¨: {batch['value_label']}
- íŠ¸ë Œë“œ: {batch['trend']}
- ì•ˆì •ì„±: {batch['stability']}
- ê²½ê³  ìˆ˜ì¤€: {batch['alert_level']}
"""
        
        context += f"\n## ì‚¬ìš©ì ì§ˆë¬¸\n{user_query}\n"
        context += """
## ë‹µë³€ ê°€ì´ë“œë¼ì¸
1. ì œì¡°ì—… í˜„ì¥ ë‹´ë‹¹ì ê´€ì ì—ì„œ ì‹¤ìš©ì ì¸ ë‹µë³€ ì œê³µ
2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë°ì´í„° ê¸°ë°˜ ë¶„ì„
3. ì¦‰ì‹œ ì¡°ì¹˜ì‚¬í•­ê³¼ ì¥ê¸° ê°œì„ ì‚¬í•­ êµ¬ë¶„
4. ì•ˆì „ê³¼ ê´€ë ¨ëœ ë¬¸ì œëŠ” ìš°ì„ ìˆœìœ„ë¡œ ì²˜ë¦¬
5. í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ë‹µë³€
6. í•„ìš”ì‹œ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„± í–¥ìƒ

ìœ„ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì „ë¬¸ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        
        return context
    
    def call_openai_api(self, prompt):
        """OpenAI API í˜¸ì¶œ"""
        try:
            import openai
            
            # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            client = openai.OpenAI(api_key=self.openai_api_key)
            
            response = client.chat.completions.create(
                # model="gpt-3.5-turbo",
                model= "gpt-4.1-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì›Œí„°íŒí”„ ì˜¨ë„ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œì¡°ì—… í˜„ì¥ì˜ ì‹¤ë¬´ì§„ì—ê²Œ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"âŒ OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\nğŸ’¡ ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:\n- API í‚¤ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸\n- ì¸í„°ë„· ì—°ê²° ìƒíƒœ í™•ì¸\n- OpenAI ê³„ì • ì”ì•¡ í™•ì¸"
    
    def call_ollama_api(self, prompt):
        """Ollama API í˜¸ì¶œ"""
        try:
            url = "http://localhost:11434/api/generate"
            data = {
                "model": self.ollama_model,
                "prompt": prompt,
                "stream": False
            }
            
            response = requests.post(url, json=data, timeout=60)
            
            if response.status_code == 200:
                return response.json()["response"]
            else:
                return f"âŒ Ollama API í˜¸ì¶œ ì‹¤íŒ¨: HTTP {response.status_code}\n\nğŸ’¡ ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:\n- Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸ (ollama serve)\n- ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ollama pull {self.ollama_model})"
                
        except requests.exceptions.ConnectionError:
            return f"âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nğŸ’¡ ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:\n- Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n- 'ollama serve' ëª…ë ¹ìœ¼ë¡œ ì„œë²„ ì‹œì‘\n- http://localhost:11434 ì ‘ì† ê°€ëŠ¥í•œì§€ í™•ì¸"
        except requests.exceptions.Timeout:
            return f"âŒ Ollama API ì‘ë‹µ ì‹œê°„ ì´ˆê³¼\n\nğŸ’¡ ëª¨ë¸ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì„œë²„ ì„±ëŠ¥ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        except Exception as e:
            return f"âŒ Ollama API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    def get_llm_response(self, user_query):
        """LLMì„ í†µí•œ ì‘ë‹µ ìƒì„±"""
        if not self.llm_provider:
            return "ğŸš« LLM ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”."
        
        # ì»¨í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
        context_prompt = self.create_context_prompt(user_query)
        
        # LLM í˜¸ì¶œ
        if self.llm_provider == "openai":
            if not self.openai_api_key:
                return "ğŸ”‘ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            return self.call_openai_api(context_prompt)
        
        elif self.llm_provider == "ollama":
            return self.call_ollama_api(context_prompt)
        
        else:
            return "âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µìì…ë‹ˆë‹¤."

def load_and_analyze_csv(uploaded_file):
    """CSV íŒŒì¼ ë¡œë“œ ë° ë¶„ì„"""
    try:
        # CSV ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
        preview_df = pd.read_csv(uploaded_file)
        st.sidebar.write("ğŸ“‹ **íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°**")
        st.sidebar.dataframe(preview_df.head(3))
        
        # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
        uploaded_file.seek(0)
        
        # ë¶„ì„ê¸° ìƒì„± ë° ë°ì´í„° ë¡œë“œ
        analyzer = WaterPumpAnalyzer()
        
        if analyzer.load_data(uploaded_file=uploaded_file):
            st.sidebar.write(f"ğŸ“Š **ë°ì´í„° ì •ë³´**")
            st.sidebar.write(f"ë ˆì½”ë“œ ìˆ˜: {len(analyzer.data)}")
            st.sidebar.write(f"ì˜¨ë„ ë²”ìœ„: {analyzer.data['value'].min():.1f}Â°C ~ {analyzer.data['value'].max():.1f}Â°C")
            
            # ì˜¨ë„ ë¶„ì„ ì‹¤í–‰
            analyzer.analyze_temperature_characteristics()
            
            # ì±—ë´‡ìš© ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            chatbot_data = {
                'metadata': {
                    'analysis_date': datetime.now().isoformat(),
                    'total_batches': len(analyzer.analyzed_data),
                    'window_size': 100,
                    'data_source': 'uploaded_csv_file'
                },
                'analysis_results': analyzer.analyzed_data
            }
            
            # ì±—ë´‡ì— ë°ì´í„° ì„¤ì •
            st.session_state.chatbot.data = chatbot_data
            st.session_state.chatbot.analyze_data()
            
            # ìë™ìœ¼ë¡œ water_pump_data í´ë”ì— ì €ì¥
            import os
            data_folder = 'water_pump_data'
            if not os.path.exists(data_folder):
                os.makedirs(data_folder)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            json_filename = f"llm_chatbot_analysis_{timestamp}.json"
            json_path = os.path.join(data_folder, json_filename)
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(chatbot_data, f, ensure_ascii=False, indent=2)
            
            st.sidebar.info(f"ğŸ“ ë¶„ì„ ê²°ê³¼ê°€ {json_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            return True
        else:
            st.sidebar.error("âŒ CSV íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        st.sidebar.error(f"âŒ CSV ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return False

def create_sample_data_for_chatbot():
    """ì±—ë´‡ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    try:
        # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        sample_data = []
        base_time = datetime.now()
        
        for i in range(500):
            timestamp = base_time + pd.Timedelta(minutes=i*10)
            if i < 100:
                temp = 45 + np.random.normal(0, 2)
            elif i < 200:
                temp = 60 + np.random.normal(0, 3)
            elif i < 300:
                temp = 75 + np.random.normal(0, 5)
            elif i < 400:
                temp = 85 + np.random.normal(0, 4)
            else:
                temp = 50 + np.random.normal(0, 2)
            
            sample_data.append({
                'timestamp': timestamp,
                'value': max(20, min(100, temp))
            })
        
        analyzer = WaterPumpAnalyzer()
        analyzer.load_data(data=sample_data)
        analyzer.analyze_temperature_characteristics()
        
        # ì±—ë´‡ìš© ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        chatbot_data = {
            'metadata': {
                'analysis_date': datetime.now().isoformat(),
                'total_batches': len(analyzer.analyzed_data),
                'window_size': 100,
                'data_source': 'sample_data'
            },
            'analysis_results': analyzer.analyzed_data
        }
        
        # ì±—ë´‡ì— ë°ì´í„° ì„¤ì •
        st.session_state.chatbot.data = chatbot_data
        st.session_state.chatbot.analyze_data()
        
        # ìë™ìœ¼ë¡œ water_pump_data í´ë”ì— ì €ì¥
        import os
        data_folder = 'water_pump_data'
        if not os.path.exists(data_folder):
            os.makedirs(data_folder)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        json_filename = f"llm_sample_data_analysis_{timestamp}.json"
        json_path = os.path.join(data_folder, json_filename)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(chatbot_data, f, ensure_ascii=False, indent=2)
        
        st.sidebar.info(f"ğŸ“ ìƒ˜í”Œ ë°ì´í„° ë¶„ì„ ê²°ê³¼ê°€ {json_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        st.sidebar.error(f"âŒ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")

def display_upload_guide():
    """ë°ì´í„° ì—…ë¡œë“œ ê°€ì´ë“œ í‘œì‹œ"""
    st.info("ğŸ“¤ ì‚¬ì´ë“œë°”ì—ì„œ LLM ëª¨ë¸ì„ ì„¤ì •í•˜ê³  ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    
    st.subheader("ğŸ¤– ì§€ì›í•˜ëŠ” LLM ëª¨ë¸")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸŒ OpenAI GPT")
        st.markdown("""
        **âœ… ì¥ì :**
        - ë›°ì–´ë‚œ í•œêµ­ì–´ ì²˜ë¦¬ ëŠ¥ë ¥
        - ë¹ ë¥¸ ì‘ë‹µ ì†ë„
        - ì¼ê´€ëœ ê³ í’ˆì§ˆ ë‹µë³€
        
        **ğŸ“‹ ì„¤ì • ë°©ë²•:**
        1. OpenAI API í‚¤ ë°œê¸‰
        2. ì‚¬ì´ë“œë°”ì—ì„œ OpenAI ì„ íƒ
        3. API í‚¤ ì…ë ¥
        """)
        
        st.info("ğŸ’¡ **API í‚¤ ë°œê¸‰:** https://platform.openai.com/api-keys")
    
    with col2:
        st.subheader("ğŸ  Local Ollama")
        st.markdown("""
        **âœ… ì¥ì :**
        - ë¡œì»¬ ì‹¤í–‰ìœ¼ë¡œ ë°ì´í„° ë³´ì•ˆ
        - API ë¹„ìš© ì—†ìŒ
        - ì˜¤í”„ë¼ì¸ ì‚¬ìš© ê°€ëŠ¥
        
        **ğŸ“‹ ì„¤ì • ë°©ë²•:**
        1. Ollama ì„¤ì¹˜
        2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        3. ì„œë²„ ì‹¤í–‰: `ollama serve`
        """)
        
        st.info("ğŸ’¡ **Ollama ì„¤ì¹˜:** https://ollama.ai/download")
    
    st.subheader("ğŸ“ ì§€ì›í•˜ëŠ” ë°ì´í„° í˜•ì‹")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“Š CSV íŒŒì¼ í˜•ì‹")
        st.code("""
timestamp,value
2024-01-01 10:00:00,45.2
2024-01-01 10:10:00,46.1
2024-01-01 10:20:00,47.3
        """, language="csv")
    
    with col2:
        st.subheader("ğŸ“„ JSON íŒŒì¼ í˜•ì‹")
        st.code("""
{
  "metadata": {...},
  "analysis_results": [...]
}
        """, language="json")

def setup_llm_provider():
    """LLM ì œê³µì ì„¤ì • UI"""
    st.sidebar.header("ğŸ¤– LLM ëª¨ë¸ ì„¤ì •")
    
    # LLM ì œê³µì ì„ íƒ
    llm_provider = st.sidebar.radio(
        "LLM ëª¨ë¸ ì„ íƒ:",
        ["ì„ íƒ ì•ˆí•¨", "OpenAI GPT", "Local Ollama"],
        key="llm_provider"
    )
    
    if llm_provider == "OpenAI GPT":
        st.sidebar.subheader("ğŸ”‘ OpenAI ì„¤ì •")
        
        # API í‚¤ ì…ë ¥
        api_key = os.getenv('OPENAI_API_KEY')
        #api_key = st.sidebar.text_input(
        #    "API Key:",
        #    type="password",
        #    placeholder="sk-...",
        #    placeholder="****",
        #    help="OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”"
        #)
        
        if api_key:
            # OpenAI íŒ¨í‚¤ì§€ í™•ì¸
            try:
                import openai
                st.session_state.chatbot.set_llm_provider("openai", api_key=api_key)
                st.sidebar.success("âœ… OpenAI ì„¤ì • ì™„ë£Œ!")
                return True
            except ImportError:
                st.sidebar.error("âŒ OpenAI íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                st.sidebar.code("pip install openai")
                return False
        else:
            st.sidebar.warning("ğŸ”‘ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return False
    
    elif llm_provider == "Local Ollama":
        st.sidebar.subheader("ğŸ  Ollama ì„¤ì •")
        
        # ëª¨ë¸ ì„ íƒ
        ollama_model = st.sidebar.selectbox(
            "ëª¨ë¸ ì„ íƒ:",
            ["cogito:14b", "cogito:8b","gemma3:12b", "deepseek-r1:14b", "mistral", "llama3:latest"],
            help="ì‚¬ìš©í•  Ollama ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”"
        )
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        if st.sidebar.button("ğŸ” ì—°ê²° í…ŒìŠ¤íŠ¸"):
            try:
                response = requests.get("http://localhost:11434/api/version", timeout=5)
                if response.status_code == 200:
                    st.session_state.chatbot.set_llm_provider("ollama", model=ollama_model)
                    st.sidebar.success("âœ… Ollama ì—°ê²° ì„±ê³µ!")
                    return True
                else:
                    st.sidebar.error("âŒ Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜")
                    return False
            except:
                st.sidebar.error("âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.sidebar.info("ğŸ’¡ 'ollama serve' ëª…ë ¹ìœ¼ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
                return False
        
        # ìë™ ì„¤ì • (ì´ë¯¸ ì„¤ì •ëœ ê²½ìš°)
        if hasattr(st.session_state.chatbot, 'ollama_model') and st.session_state.chatbot.ollama_model:
            return True
    
    return False

def main():
    st.title("ğŸ¤– ì›Œí„°íŒí”„ AI ë¶„ì„ ì±—ë´‡ (LLM)")
    st.markdown("### OpenAI GPT & Local Ollama ì§€ì›")
    
    # ì±—ë´‡ ì´ˆê¸°í™”
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = LLMWaterPumpChatbot()
    
    if 'data_loaded' not in st.session_state:
        st.session_state.data_loaded = False
    
    if 'llm_configured' not in st.session_state:
        st.session_state.llm_configured = False
    
    # ì‚¬ì´ë“œë°” - LLM ì„¤ì •
    llm_configured = setup_llm_provider()
    st.session_state.llm_configured = llm_configured
    
    # ì‚¬ì´ë“œë°” - ë°ì´í„° ì—…ë¡œë“œ
    with st.sidebar:
        st.subheader("ğŸ“‚ ë°ì´í„° ì—…ë¡œë“œ")
        
        if not llm_configured:
            st.warning("âš ï¸ ë¨¼ì € LLM ëª¨ë¸ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        
        # ì—…ë¡œë“œ ë°©ì‹ ì„ íƒ
        upload_method = st.radio(
            "ë°ì´í„° ì…ë ¥ ë°©ì‹:",
            ["CSV íŒŒì¼", "JSON íŒŒì¼", "ìƒ˜í”Œ ë°ì´í„°"],
            disabled=not llm_configured
        )
        
        if upload_method == "CSV íŒŒì¼" and llm_configured:
            uploaded_file = st.file_uploader(
                "CSV íŒŒì¼ì„ ë“œë˜ê·¸í•˜ê±°ë‚˜ ì„ íƒí•˜ì„¸ìš”",
                type=['csv'],
                help="timestamp, value ì»¬ëŸ¼ì´ í¬í•¨ëœ CSV íŒŒì¼"
            )
            
            if uploaded_file and not st.session_state.data_loaded:
                if st.button("ğŸ”„ CSV ë¶„ì„ ì‹¤í–‰"):
                    with st.spinner("CSV ë°ì´í„° ë¶„ì„ ì¤‘..."):
                        success = load_and_analyze_csv(uploaded_file)
                        if success:
                            st.session_state.data_loaded = True
                            st.success("âœ… CSV ë¶„ì„ ì™„ë£Œ!")
        
        elif upload_method == "JSON íŒŒì¼" and llm_configured:
            uploaded_file = st.file_uploader(
                "JSON íŒŒì¼ì„ ë“œë˜ê·¸í•˜ê±°ë‚˜ ì„ íƒí•˜ì„¸ìš”",
                type=['json'],
                help="ì›Œí„°íŒí”„ ì˜¨ë„ ë¶„ì„ JSON íŒŒì¼"
            )
            
            if uploaded_file and not st.session_state.data_loaded:
                if st.session_state.chatbot.load_json_data(uploaded_file):
                    st.session_state.data_loaded = True
                    st.success("âœ… JSON ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
        
        elif upload_method == "ìƒ˜í”Œ ë°ì´í„°" and llm_configured:
            if st.button("ğŸ² ìƒ˜í”Œ ë°ì´í„° ìƒì„±") and not st.session_state.data_loaded:
                with st.spinner("ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘..."):
                    create_sample_data_for_chatbot()
                    st.session_state.data_loaded = True
                    st.success("âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
        
        # ë°ì´í„° ì •ë³´ í‘œì‹œ
        if st.session_state.data_loaded and st.session_state.chatbot.data:
            st.write("---")
            st.write("ğŸ“Š **ë¡œë“œëœ ë°ì´í„° ì •ë³´**")
            metadata = st.session_state.chatbot.data.get('metadata', {})
            st.write(f"ì´ ë°°ì¹˜: {metadata.get('total_batches', 'N/A')}ê°œ")
            st.write(f"ë°ì´í„° ì†ŒìŠ¤: {metadata.get('data_source', 'N/A')}")
            
            cache = st.session_state.chatbot.analysis_cache
            if cache:
                st.write(f"í‰ê·  ì˜¨ë„: {cache['avg_temperature']:.1f}Â°C")
                critical_count = cache['alert_counts']['ìœ„í—˜'] + cache['alert_counts']['ì£¼ì˜']
                st.write(f"ìœ„í—˜/ì£¼ì˜: {critical_count}ê°œ")
                
                # LLM ëª¨ë¸ ì •ë³´
                st.write("---")
                st.write("ğŸ¤– **LLM ëª¨ë¸ ì •ë³´**")
                if st.session_state.chatbot.llm_provider == "openai":

                    st.write("ğŸŒ OpenAI gpt-4.1-mini" ) 

                elif st.session_state.chatbot.llm_provider == "ollama":
                    st.write(f"ğŸ  Ollama ({st.session_state.chatbot.ollama_model})")
    
    # ë©”ì¸ ì˜ì—­
    if not llm_configured:
        display_upload_guide()
        return
    
    if not st.session_state.data_loaded:
        st.info("ğŸ“¤ ì‚¬ì´ë“œë°”ì—ì„œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return
    
    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    st.subheader("ğŸ’¬ LLM ê¸°ë°˜ AI ë¶„ì„ ì±„íŒ…")
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    if 'messages' not in st.session_state:
        welcome_msg = "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì›Œí„°íŒí”„ ì˜¨ë„ ë°ì´í„° ë¶„ì„ ì „ë¬¸ AIì…ë‹ˆë‹¤. ğŸŒ¡ï¸\n\n"
        if st.session_state.chatbot.llm_provider == "openai":
            welcome_msg += "ğŸŒ **OpenAI GPT**ë¡œ êµ¬ë™ë˜ì–´ ê³ í’ˆì§ˆ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.\n"
        elif st.session_state.chatbot.llm_provider == "ollama":
            welcome_msg += f"ğŸ  **Local Ollama ({st.session_state.chatbot.ollama_model})**ë¡œ êµ¬ë™ë˜ì–´ ë³´ì•ˆì´ ë³´ì¥ë©ë‹ˆë‹¤.\n"
        welcome_msg += "\nì—…ë¡œë“œëœ JSON ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë¶„ì„ê³¼ ì¡°ì–¸ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"
        
        st.session_state.messages = [
            {"role": "assistant", "content": welcome_msg}
        ]
    
    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # LLM ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤– AIê°€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                response = st.session_state.chatbot.get_llm_response(prompt)
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
    
    # í€µ ì•¡ì…˜ ë²„íŠ¼
    st.subheader("ğŸš€ ë¹ ë¥¸ ë¶„ì„ (LLM ê¸°ë°˜)")
    
    col1, col2, col3 = st.columns(3)
    
    quick_queries = {
        "ì „ì²´ í˜„í™©": "í˜„ì¬ ì›Œí„°íŒí”„ì˜ ì „ë°˜ì ì¸ ì˜¨ë„ ìƒíƒœëŠ” ì–´ë–¤ê°€ìš”? ì£¼ìš” í†µê³„ì™€ í•¨ê»˜ ì¢…í•©ì ì¸ í‰ê°€ë¥¼ í•´ì£¼ì„¸ìš”.",
        "ìœ„í—˜ ë¶„ì„": "í˜„ì¬ ìœ„í—˜í•˜ê±°ë‚˜ ì£¼ì˜ê°€ í•„ìš”í•œ ìƒí™©ì´ ìˆë‚˜ìš”? êµ¬ì²´ì ì¸ ë°°ì¹˜ì™€ ëŒ€ì‘ ë°©ì•ˆì„ ì•Œë ¤ì£¼ì„¸ìš”.",
        "ì •ë¹„ ì¡°ì–¸": "ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡ ì •ë¹„ ê³„íšì„ ì„¸ì›Œì£¼ì„¸ìš”. ìš°ì„ ìˆœìœ„ì™€ êµ¬ì²´ì ì¸ ì ê²€ í•­ëª©ì„ í¬í•¨í•´ì£¼ì„¸ìš”."
    }
    
    with col1:
        if st.button("ğŸ“Š ì „ì²´ í˜„í™© (LLM)"):
            query = quick_queries["ì „ì²´ í˜„í™©"]
            st.session_state.messages.append({"role": "user", "content": query})
            with st.spinner("ğŸ¤– ë¶„ì„ ì¤‘..."):
                response = st.session_state.chatbot.get_llm_response(query)
                st.session_state.messages.append({"role": "assistant", "content": response})
            st.rerun()
    
    with col2:
        if st.button("âš ï¸ ìœ„í—˜ ë¶„ì„ (LLM)"):
            query = quick_queries["ìœ„í—˜ ë¶„ì„"]
            st.session_state.messages.append({"role": "user", "content": query})
            with st.spinner("ğŸ¤– ë¶„ì„ ì¤‘..."):
                response = st.session_state.chatbot.get_llm_response(query)
                st.session_state.messages.append({"role": "assistant", "content": response})
            st.rerun()
    
    with col3:
        if st.button("ğŸ”§ ì •ë¹„ ì¡°ì–¸ (LLM)"):
            query = quick_queries["ì •ë¹„ ì¡°ì–¸"]
            st.session_state.messages.append({"role": "user", "content": query})
            with st.spinner("ğŸ¤– ë¶„ì„ ì¤‘..."):
                response = st.session_state.chatbot.get_llm_response(query)
                st.session_state.messages.append({"role": "assistant", "content": response})
            st.rerun()
    
    # ê³ ê¸‰ ì§ˆë¬¸ ì˜ˆì‹œ
    st.subheader("ğŸ’¡ ê³ ê¸‰ ì§ˆë¬¸ ì˜ˆì‹œ")
    
    example_queries = [
        "ì˜¨ë„ íŠ¸ë Œë“œë¥¼ ë³´ë©´ ì•ìœ¼ë¡œ ì–´ë–¤ ë¬¸ì œê°€ ì˜ˆìƒë˜ë‚˜ìš”?",
        "ì—ë„ˆì§€ íš¨ìœ¨ì„ ë†’ì´ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.",
        "í˜„ì¬ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ìš´ì˜ ì˜¨ë„ ë²”ìœ„ëŠ” ì–´ë–»ê²Œ ì„¤ì •í•´ì•¼ í• ê¹Œìš”?",
        "ê³„ì ˆë³„ ì˜¨ë„ ë³€í™”ì— ëŒ€ë¹„í•œ ì˜ˆë°© ì¡°ì¹˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ë¹„ìŠ·í•œ íŒ¨í„´ì˜ ë°°ì¹˜ë“¤ì„ ê·¸ë£¹í™”í•˜ì—¬ ìš´ì˜ ì „ëµì„ ì„¸ì›Œì£¼ì„¸ìš”.",
        "í˜„ì¬ ì„¤ì •ëœ ê²½ê³  ì„ê³„ê°’ì´ ì ì ˆí•œì§€ í‰ê°€í•´ì£¼ì„¸ìš”."
    ]
    
    cols = st.columns(2)
    for i, query in enumerate(example_queries):
        col = cols[i % 2]
        if col.button(f"ğŸ’­ {query[:30]}...", key=f"example_{i}"):
            st.session_state.messages.append({"role": "user", "content": query})
            with st.spinner("ğŸ¤– ë¶„ì„ ì¤‘..."):
                response = st.session_state.chatbot.get_llm_response(query)
                st.session_state.messages.append({"role": "assistant", "content": response})
            st.rerun()

if __name__ == "__main__":
    main()

## ì‹¤í–‰ ë°©ë²• (CLIì—ì„œ) 
## streamlit run chatbot_implementation_openai.py
## 
